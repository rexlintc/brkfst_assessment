{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cabcf835-0152-4b37-a46e-43a0ed3c78c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bec491-c59f-487c-9959-8ce89e3ebb18",
   "metadata": {},
   "source": [
    "# Parse data for network graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a77fae5-cd22-4a36-927b-66e60a373f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_instagram_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    rows = []\n",
    "    # Add the owner as a node\n",
    "    rows.append({\n",
    "        \"source\": data[\"owner_id\"],\n",
    "        \"target\": None,\n",
    "        \"type\": \"owner\",\n",
    "        \"username\": data[\"username\"],\n",
    "        \"fullname\": data[\"fullname\"],\n",
    "        \"post_id\": data[\"post_id\"],\n",
    "        \"likes\": data[\"likes\"],\n",
    "        \"count_followed\": data[\"user\"][\"count_followed\"]\n",
    "    })\n",
    "    \n",
    "    # Add tagged users as nodes and edges\n",
    "    for user in data[\"tagged_users\"]:\n",
    "        rows.append({\n",
    "            \"source\": data[\"owner_id\"],\n",
    "            \"target\": user[\"id\"],\n",
    "            \"type\": \"tagged\",\n",
    "            \"username\": user[\"username\"],\n",
    "            \"fullname\": user[\"full_name\"],\n",
    "            \"post_id\": data[\"post_id\"],\n",
    "            \"likes\": data[\"likes\"],\n",
    "            \"count_followed\": None,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def process_all_json_files(base_path, parser):\n",
    "    all_data = []\n",
    "    for username in os.listdir(base_path):\n",
    "        user_path = os.path.join(base_path, username)\n",
    "        if os.path.isdir(user_path):\n",
    "            for file in os.listdir(user_path):\n",
    "                if file.endswith('.json'):\n",
    "                    file_path = os.path.join(user_path, file)\n",
    "                    df = parser(file_path)\n",
    "                    all_data.append(df)\n",
    "    \n",
    "    return pd.concat(all_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9433c103-392a-47a1-9811-14cfa670fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'scraped_data/instagram'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab2bc19-cdc9-42cd-9517-b3e598182e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = process_all_json_files(base_path, parse_instagram_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a601ab10-9f46-4639-8dbb-a5a4280ac079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep unique account owners and their tagged accounts\n",
    "cleaned_combined_df = combined_df.drop_duplicates(subset=['source', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2b00917-6f0a-4f42-9329-c808e5af2741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-19.0.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Downloading pyarrow-19.0.1-cp312-cp312-macosx_12_0_arm64.whl (30.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-19.0.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bc717cc-38bb-43ef-9d98-83b11d5074d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_combined_df.to_parquet('parsed_data/network_graph_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a873a20-4cfb-471a-a7d3-2d13a27e9d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_combined_df.to_csv('parsed_data/network_graph_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4cd9a-20ca-44bd-adbd-e6c1751387b0",
   "metadata": {},
   "source": [
    "# Parse data for virality predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77394c0c-5129-43eb-bfc6-511a1b1d8b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_instagram_json_for_model_training(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    rows = []\n",
    "    # Add the owner as a node\n",
    "    rows.append({\n",
    "        \"media_id\": data[\"media_id\"],\n",
    "        \"caption\": data[\"description\"],\n",
    "        \"width\": data[\"width\"],\n",
    "        \"height\": data[\"height\"],\n",
    "        \"likes\": data[\"likes\"],\n",
    "        \"count\": data[\"count\"],\n",
    "        \"filename\": os.path.splitext(os.path.basename(file_path))[0],\n",
    "    })\n",
    "    \n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e905c10-6649-400a-bdd5-9fc9cbd2bb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_meta_df = process_all_json_files(base_path, parse_instagram_json_for_model_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d400118a-8707-45a6-8eba-acbee23e0334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw data as parquet\n",
    "model_training_meta_df.to_parquet('model_training_data_raw.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "66531697-318f-451f-9cd0-38835175adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(column):\n",
    "    min_val = column.min()\n",
    "    max_val = column.max()\n",
    "    if max_val == min_val:\n",
    "        return column * 0  # Handle case where all values are the same\n",
    "    return (column - min_val) / (max_val - min_val)\n",
    "\n",
    "# Normalize data\n",
    "model_training_meta_df['virality_score'] = normalize(model_training_meta_df['likes'])\n",
    "model_training_meta_df['width'] = normalize(model_training_meta_df['width'])\n",
    "model_training_meta_df['height'] = normalize(model_training_meta_df['height'])\n",
    "model_training_meta_df['count'] = normalize(model_training_meta_df['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "07c4e77b-1083-4bed-85b3-fb01d067d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d0a623e8-30c1-4c63-8160-e1953f0c864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "975c5b47-f702-4324-b34e-3504d9d9d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embedding(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the embeddings from the last hidden state\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    \n",
    "    # Average the token embeddings to get a single vector for the entire text\n",
    "    sentence_embedding = torch.mean(embeddings, dim=1)\n",
    "    \n",
    "    return sentence_embedding.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6620fab4-7699-46ef-9556-0c930c51acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_embedding = []\n",
    "for caption in model_training_meta_df['caption']:\n",
    "    embedding = get_bert_embedding(caption)\n",
    "    captions_embedding.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a5d33ce6-e26b-4436-8ca5-494bcb7d00e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_meta_df['embedded_caption'] = captions_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3c96be6d-c7af-423f-97d3-209d4ee95e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['media_id', 'caption', 'width', 'height', 'likes', 'count', 'filename',\n",
       "       'virality_score', 'embbeded_caption'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_training_meta_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4be0a48b-cbbc-41ca-8af3-5b6ef3669642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def extract_embeddings(image_folder, model_name='resnet50', batch_size=32):\n",
    "    \"\"\"\n",
    "    Extract embeddings from images using a pre-trained model\n",
    "    \n",
    "    Args:\n",
    "        image_folder: Path to folder containing JPG images\n",
    "        model_name: 'resnet50' or 'vgg16'\n",
    "        batch_size: Number of images to process at once\n",
    "        \n",
    "    Returns:\n",
    "        embeddings: numpy array of embeddings\n",
    "        image_paths: list of image paths\n",
    "    \"\"\"\n",
    "    # Choose model\n",
    "    if model_name == 'resnet50':\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        # Remove the final fully connected layer\n",
    "        model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "        embedding_size = 2048\n",
    "    elif model_name == 'vgg16':\n",
    "        model = models.vgg16(pretrained=True)\n",
    "        # Use features part of VGG (before fully connected layers)\n",
    "        model = model.features\n",
    "        # Add global average pooling to get fixed size output\n",
    "        model = torch.nn.Sequential(\n",
    "            model,\n",
    "            torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        embedding_size = 512\n",
    "    else:\n",
    "        raise ValueError(\"Model must be 'resnet50' or 'vgg16'\")\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define image preprocessing\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Get all jpg images\n",
    "    image_paths = get_all_img_paths(image_folder)\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    # Process images in batches\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        batch_tensors = []\n",
    "        \n",
    "        for img_path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_tensor = preprocess(img)\n",
    "                batch_tensors.append(img_tensor)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not batch_tensors:\n",
    "            continue\n",
    "            \n",
    "        # Stack tensors into a batch\n",
    "        batch = torch.stack(batch_tensors).to(device)\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch)\n",
    "            \n",
    "        # Reshape and convert to numpy\n",
    "        batch_embeddings = batch_embeddings.squeeze().cpu().numpy()\n",
    "        \n",
    "        # Handle single image case\n",
    "        if len(batch_tensors) == 1:\n",
    "            batch_embeddings = batch_embeddings.reshape(1, -1)\n",
    "            \n",
    "        embeddings.append(batch_embeddings)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    if embeddings:\n",
    "        embeddings = np.vstack(embeddings)\n",
    "    else:\n",
    "        embeddings = np.array([])\n",
    "    \n",
    "    return embeddings, image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06fd1c1c-b6d6-4ea2-9057-d5173dbc24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_img_paths(base_path):\n",
    "    img_paths = []\n",
    "    for username in os.listdir(base_path):\n",
    "        user_path = os.path.join(base_path, username)\n",
    "        if os.path.isdir(user_path):\n",
    "            for file in os.listdir(user_path):\n",
    "                if file.endswith('.jpg'):\n",
    "                    file_path = os.path.join(user_path, file)\n",
    "                    img_paths.append(file_path)\n",
    "    \n",
    "    return img_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b9dfc9a-59ba-480a-86a9-784ec6d0b050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 162 embeddings with shape (162, 2048)\n"
     ]
    }
   ],
   "source": [
    "image_folder = \"scraped_data/instagram/\"\n",
    "\n",
    "# Extract embeddings using ResNet50 (faster)\n",
    "embeddings, image_paths = extract_embeddings(image_folder, model_name='resnet50')\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings with shape {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "735c03e5-f310-49eb-ab01-f19541b80f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to file\n",
    "np.save(\"image_embeddings.npy\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ee71b97-8db1-41ae-b2a2-dbdf0cf3362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load('image_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a55513e-7cf6-41f3-aabb-db4f92531fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save image paths for reference\n",
    "with open(\"image_paths.txt\", \"w\") as f:\n",
    "    for path in image_paths:\n",
    "        f.write(f\"{path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "65d33975-85ff-497f-b9c5-c210e0fe032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = get_all_img_paths(base_path)\n",
    "file_name = [os.path.basename(path) for path in image_paths if path.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85844362-657b-4fd9-ac47-ad7ebba9d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = pd.DataFrame(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fe82c898-babb-4ab5-8217-0440c1ea395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df['filename'] = file_name\n",
    "embeddings_df['filename'] = embeddings_df['filename'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ba660f37-39a1-40a1-a722-58528c0a5a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_meta_df['filename'] = model_training_meta_df['filename'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "debbb252-b5d2-4d00-b042-546ac41ad9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = pd.merge(embeddings_df, model_training_meta_df, on='filename')\n",
    "joined_df = joined_df.drop(['filename', 'media_id', 'caption'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3ac1de41-e563-43cb-964d-ee37dc2b84c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle caption embedding\n",
    "array_df = pd.DataFrame(joined_df['embbeded_caption'].tolist(), \n",
    "                        # index=joined_df.index, \n",
    "                        columns=[f'caption_{i}' for i in range(len(joined_df['embbeded_caption'].iloc[0]))])\n",
    "                        \n",
    "# Combine with original dataframe\n",
    "joined_df = pd.concat([joined_df.drop('embbeded_caption', axis=1), array_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6507fa2f-82db-45f2-a1d5-8eedd36368c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.percentile(joined_df['virality_score'], 75)  # Top 25% considered viral\n",
    "joined_df['virality_label'] = (joined_df['virality_score'] >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "646f2d85-547b-4a34-96b8-30860bd7535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = joined_df.drop('virality_score', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fd28757d-343f-4330-851e-9e2398446bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rex/brkfst_assessment/venv/lib/python3.12/site-packages/pandas/io/parquet.py:190: UserWarning: The DataFrame has column names of mixed type. They will be converted to strings and not roundtrip correctly.\n",
      "  table = self.api.Table.from_pandas(df, **from_pandas_kwargs)\n"
     ]
    }
   ],
   "source": [
    "joined_df.to_parquet('model_training_data.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
